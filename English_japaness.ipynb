{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lbW2YAjJH8P",
        "outputId": "c8df1b70-c60c-461e-da4b-934a7dcaccf3"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t## tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'jpn.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-japan.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-japan.pkl\n",
            "[Go] => [行け。]\n",
            "[Go] => [行きなさい。]\n",
            "[Hi] => [こんにちは。]\n",
            "[Hi] => [もしもし。]\n",
            "[Hi] => [やっほー。]\n",
            "[Hi] => [こんにちは！]\n",
            "[Run] => [走れ！]\n",
            "[Run] => [走れ。]\n",
            "[Run] => [走って！]\n",
            "[Who] => [誰？]\n",
            "[Wow] => [すごい！]\n",
            "[Wow] => [ワォ！]\n",
            "[Wow] => [わぉ！]\n",
            "[Wow] => [おー！]\n",
            "[Fire] => [火事だ！]\n",
            "[Fire] => [火事！]\n",
            "[Fire] => [撃て！]\n",
            "[Help] => [助けて！]\n",
            "[Help] => [助けてくれ！]\n",
            "[Hide] => [隠れろ。]\n",
            "[Jump] => [飛び越えろ！]\n",
            "[Jump] => [跳べ！]\n",
            "[Jump] => [飛び降りろ！]\n",
            "[Jump] => [飛び跳ねて！]\n",
            "[Jump] => [ジャンプして！]\n",
            "[Jump] => [跳べ！]\n",
            "[Jump] => [飛び跳ねて！]\n",
            "[Jump] => [ジャンプして！]\n",
            "[Stop] => [やめろ！]\n",
            "[Stop] => [止まれ！]\n",
            "[Wait] => [待って！]\n",
            "[Go on] => [続けて。]\n",
            "[Go on] => [進んで。]\n",
            "[Go on] => [進め。]\n",
            "[Go on] => [続けろ。]\n",
            "[Hello] => [こんにちは。]\n",
            "[Hello] => [もしもし。]\n",
            "[Hello] => [こんにちは！]\n",
            "[Hurry] => [急げ！]\n",
            "[I see] => [なるほど。]\n",
            "[I see] => [なるほどね。]\n",
            "[I see] => [わかった。]\n",
            "[I see] => [わかりました。]\n",
            "[I see] => [そうですか。]\n",
            "[I see] => [そうなんだ。]\n",
            "[I see] => [そっか。]\n",
            "[I try] => [頑張ってみる。]\n",
            "[I try] => [やってみる。]\n",
            "[I try] => [試してみる。]\n",
            "[I try] => [やってみよう！]\n",
            "[I try] => [トライしてみる。]\n",
            "[I won] => [俺の勝ちー！]\n",
            "[I won] => [勝ったぁ！]\n",
            "[I won] => [勝ったぞ！]\n",
            "[I won] => [私の勝ち！]\n",
            "[I won] => [私が勝ち！]\n",
            "[Oh no] => [なんてこった！]\n",
            "[Oh no] => [なんてことだ！]\n",
            "[Oh no] => [しまった！]\n",
            "[Oh no] => [あー、しまった！]\n",
            "[Oh no] => [うわ、しまった！]\n",
            "[Oh no] => [何てことだ！]\n",
            "[Relax] => [落ち着いて。]\n",
            "[Relax] => [くつろいで。]\n",
            "[Relax] => [リラックスして。]\n",
            "[Relax] => [楽にしてください。]\n",
            "[Shoot] => [撃て！]\n",
            "[Smile] => [はい、チーズ。]\n",
            "[Smile] => [にっこり笑って。]\n",
            "[Cheers] => [乾杯！]\n",
            "[Freeze] => [動くな！]\n",
            "[Get up] => [起きなさい！]\n",
            "[Get up] => [起きなさい。]\n",
            "[Get up] => [起きろ！]\n",
            "[Go now] => [さあ、行っといで。]\n",
            "[Got it] => [捕まえた。]\n",
            "[Got it] => [分かった！]\n",
            "[He ran] => [彼は走った。]\n",
            "[He ran] => [彼が走った。]\n",
            "[Hop in] => [乗れよ。]\n",
            "[Hop in] => [さあ乗って。]\n",
            "[Hug me] => [抱きしめて。]\n",
            "[Hug me] => [ぎゅーして。]\n",
            "[I know] => [分かってる。]\n",
            "[I know] => [分かってます。]\n",
            "[I left] => [出発した]\n",
            "[I lost] => [負けた・・・。]\n",
            "[I paid] => [払いました。]\n",
            "[I quit] => [私、辞めます。]\n",
            "[I quit] => [やめた。]\n",
            "[Im 19] => [１９歳です。]\n",
            "[Im OK] => [大丈夫ですよ。]\n",
            "[Im OK] => [私は大丈夫です。]\n",
            "[Im up] => [起きてるよ。]\n",
            "[Listen] => [聞きなさい。]\n",
            "[Listen] => [聞いて！]\n",
            "[No way] => [馬鹿な！]\n",
            "[No way] => [あり得ねぇー。]\n",
            "[No way] => [とんでもない！]\n",
            "[No way] => [とんでもございません！]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtAyQH1-J8GS",
        "outputId": "7fa5dcb7-db2c-4e02-b88d-0a524695c642"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-japan.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 20000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:19000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-japan-both.pkl')\n",
        "save_clean_data(train, 'english-japan-train.pkl')\n",
        "save_clean_data(test, 'english-japan-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-japan-both.pkl\n",
            "Saved: english-japan-train.pkl\n",
            "Saved: english-japan-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjLdp9ziKNJ8",
        "outputId": "cd9e2bc2-8207-489a-8065-1084de55ec6f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-japan-both.pkl')\n",
        "train = load_clean_sentences('english-japan-train.pkl')\n",
        "test = load_clean_sentences('english-japan-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Japan Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Japan Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4207\n",
            "English Max Length: 7\n",
            "Japan Vocabulary Size: 16407\n",
            "Japan Max Length: 5\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 5, 256)            4200192   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 7, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 7, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 7, 4207)           1081199   \n",
            "=================================================================\n",
            "Total params: 6,332,015\n",
            "Trainable params: 6,332,015\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "297/297 - 92s - loss: 4.4200 - val_loss: 3.5837\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.58371, saving model to model.h5\n",
            "Epoch 2/100\n",
            "297/297 - 84s - loss: 3.5278 - val_loss: 3.3734\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.58371 to 3.37341, saving model to model.h5\n",
            "Epoch 3/100\n",
            "297/297 - 85s - loss: 3.3295 - val_loss: 3.2483\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.37341 to 3.24833, saving model to model.h5\n",
            "Epoch 4/100\n",
            "297/297 - 85s - loss: 3.2138 - val_loss: 3.1268\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.24833 to 3.12682, saving model to model.h5\n",
            "Epoch 5/100\n",
            "297/297 - 85s - loss: 3.0814 - val_loss: 3.0251\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.12682 to 3.02513, saving model to model.h5\n",
            "Epoch 6/100\n",
            "297/297 - 85s - loss: 2.9661 - val_loss: 2.9153\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02513 to 2.91527, saving model to model.h5\n",
            "Epoch 7/100\n",
            "297/297 - 87s - loss: 2.8440 - val_loss: 2.8010\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.91527 to 2.80096, saving model to model.h5\n",
            "Epoch 8/100\n",
            "297/297 - 86s - loss: 2.7083 - val_loss: 2.6628\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.80096 to 2.66280, saving model to model.h5\n",
            "Epoch 9/100\n",
            "297/297 - 86s - loss: 2.5682 - val_loss: 2.5192\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.66280 to 2.51924, saving model to model.h5\n",
            "Epoch 10/100\n",
            "297/297 - 86s - loss: 2.4300 - val_loss: 2.3889\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.51924 to 2.38885, saving model to model.h5\n",
            "Epoch 11/100\n",
            "297/297 - 86s - loss: 2.2955 - val_loss: 2.2516\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.38885 to 2.25162, saving model to model.h5\n",
            "Epoch 12/100\n",
            "297/297 - 86s - loss: 2.1584 - val_loss: 2.1315\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.25162 to 2.13150, saving model to model.h5\n",
            "Epoch 13/100\n",
            "297/297 - 85s - loss: 2.0360 - val_loss: 2.0216\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.13150 to 2.02160, saving model to model.h5\n",
            "Epoch 14/100\n",
            "297/297 - 87s - loss: 1.9223 - val_loss: 1.9267\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.02160 to 1.92668, saving model to model.h5\n",
            "Epoch 15/100\n",
            "297/297 - 86s - loss: 1.8201 - val_loss: 1.8321\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.92668 to 1.83213, saving model to model.h5\n",
            "Epoch 16/100\n",
            "297/297 - 86s - loss: 1.7221 - val_loss: 1.7438\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.83213 to 1.74380, saving model to model.h5\n",
            "Epoch 17/100\n",
            "297/297 - 84s - loss: 1.6314 - val_loss: 1.6628\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.74380 to 1.66281, saving model to model.h5\n",
            "Epoch 18/100\n",
            "297/297 - 86s - loss: 1.5467 - val_loss: 1.5873\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.66281 to 1.58735, saving model to model.h5\n",
            "Epoch 19/100\n",
            "297/297 - 85s - loss: 1.4644 - val_loss: 1.5141\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.58735 to 1.51413, saving model to model.h5\n",
            "Epoch 20/100\n",
            "297/297 - 85s - loss: 1.3849 - val_loss: 1.4393\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51413 to 1.43934, saving model to model.h5\n",
            "Epoch 21/100\n",
            "297/297 - 86s - loss: 1.3063 - val_loss: 1.3685\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.43934 to 1.36848, saving model to model.h5\n",
            "Epoch 22/100\n",
            "297/297 - 85s - loss: 1.2287 - val_loss: 1.2991\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.36848 to 1.29913, saving model to model.h5\n",
            "Epoch 23/100\n",
            "297/297 - 85s - loss: 1.1525 - val_loss: 1.2356\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.29913 to 1.23555, saving model to model.h5\n",
            "Epoch 24/100\n",
            "297/297 - 86s - loss: 1.0814 - val_loss: 1.1743\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.23555 to 1.17434, saving model to model.h5\n",
            "Epoch 25/100\n",
            "297/297 - 87s - loss: 1.0125 - val_loss: 1.1156\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.17434 to 1.11559, saving model to model.h5\n",
            "Epoch 26/100\n",
            "297/297 - 87s - loss: 0.9475 - val_loss: 1.0552\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.11559 to 1.05522, saving model to model.h5\n",
            "Epoch 27/100\n",
            "297/297 - 85s - loss: 0.8828 - val_loss: 1.0051\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.05522 to 1.00514, saving model to model.h5\n",
            "Epoch 28/100\n",
            "297/297 - 86s - loss: 0.8242 - val_loss: 0.9508\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.00514 to 0.95080, saving model to model.h5\n",
            "Epoch 29/100\n",
            "297/297 - 86s - loss: 0.7654 - val_loss: 0.9059\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.95080 to 0.90594, saving model to model.h5\n",
            "Epoch 30/100\n",
            "297/297 - 86s - loss: 0.7112 - val_loss: 0.8605\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.90594 to 0.86046, saving model to model.h5\n",
            "Epoch 31/100\n",
            "297/297 - 86s - loss: 0.6605 - val_loss: 0.8198\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.86046 to 0.81976, saving model to model.h5\n",
            "Epoch 32/100\n",
            "297/297 - 86s - loss: 0.6122 - val_loss: 0.7793\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.81976 to 0.77929, saving model to model.h5\n",
            "Epoch 33/100\n",
            "297/297 - 86s - loss: 0.5687 - val_loss: 0.7453\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.77929 to 0.74531, saving model to model.h5\n",
            "Epoch 34/100\n",
            "297/297 - 86s - loss: 0.5290 - val_loss: 0.7142\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.74531 to 0.71415, saving model to model.h5\n",
            "Epoch 35/100\n",
            "297/297 - 85s - loss: 0.4906 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.71415 to 0.68671, saving model to model.h5\n",
            "Epoch 36/100\n",
            "297/297 - 84s - loss: 0.4580 - val_loss: 0.6548\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.68671 to 0.65481, saving model to model.h5\n",
            "Epoch 37/100\n",
            "297/297 - 84s - loss: 0.4255 - val_loss: 0.6332\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.65481 to 0.63316, saving model to model.h5\n",
            "Epoch 38/100\n",
            "297/297 - 84s - loss: 0.3990 - val_loss: 0.6114\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.63316 to 0.61144, saving model to model.h5\n",
            "Epoch 39/100\n",
            "297/297 - 84s - loss: 0.3728 - val_loss: 0.5941\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.61144 to 0.59410, saving model to model.h5\n",
            "Epoch 40/100\n",
            "297/297 - 84s - loss: 0.3506 - val_loss: 0.5761\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.59410 to 0.57610, saving model to model.h5\n",
            "Epoch 41/100\n",
            "297/297 - 84s - loss: 0.3297 - val_loss: 0.5635\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.57610 to 0.56355, saving model to model.h5\n",
            "Epoch 42/100\n",
            "297/297 - 84s - loss: 0.3115 - val_loss: 0.5470\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.56355 to 0.54704, saving model to model.h5\n",
            "Epoch 43/100\n",
            "297/297 - 84s - loss: 0.2948 - val_loss: 0.5363\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.54704 to 0.53634, saving model to model.h5\n",
            "Epoch 44/100\n",
            "297/297 - 84s - loss: 0.2810 - val_loss: 0.5290\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.53634 to 0.52896, saving model to model.h5\n",
            "Epoch 45/100\n",
            "297/297 - 85s - loss: 0.2685 - val_loss: 0.5158\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.52896 to 0.51580, saving model to model.h5\n",
            "Epoch 46/100\n",
            "297/297 - 85s - loss: 0.2568 - val_loss: 0.5101\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.51580 to 0.51007, saving model to model.h5\n",
            "Epoch 47/100\n",
            "297/297 - 84s - loss: 0.2476 - val_loss: 0.5024\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.51007 to 0.50245, saving model to model.h5\n",
            "Epoch 48/100\n",
            "297/297 - 84s - loss: 0.2381 - val_loss: 0.4972\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.50245 to 0.49724, saving model to model.h5\n",
            "Epoch 49/100\n",
            "297/297 - 84s - loss: 0.2299 - val_loss: 0.4935\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.49724 to 0.49349, saving model to model.h5\n",
            "Epoch 50/100\n",
            "297/297 - 84s - loss: 0.2228 - val_loss: 0.4875\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.49349 to 0.48748, saving model to model.h5\n",
            "Epoch 51/100\n",
            "297/297 - 84s - loss: 0.2177 - val_loss: 0.4819\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.48748 to 0.48188, saving model to model.h5\n",
            "Epoch 52/100\n",
            "297/297 - 84s - loss: 0.2109 - val_loss: 0.4791\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.48188 to 0.47910, saving model to model.h5\n",
            "Epoch 53/100\n",
            "297/297 - 84s - loss: 0.2070 - val_loss: 0.4757\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.47910 to 0.47565, saving model to model.h5\n",
            "Epoch 54/100\n",
            "297/297 - 84s - loss: 0.2032 - val_loss: 0.4779\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.47565\n",
            "Epoch 55/100\n",
            "297/297 - 84s - loss: 0.1999 - val_loss: 0.4737\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.47565 to 0.47374, saving model to model.h5\n",
            "Epoch 56/100\n",
            "297/297 - 84s - loss: 0.1965 - val_loss: 0.4719\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.47374 to 0.47185, saving model to model.h5\n",
            "Epoch 57/100\n",
            "297/297 - 85s - loss: 0.1925 - val_loss: 0.4706\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.47185 to 0.47062, saving model to model.h5\n",
            "Epoch 58/100\n",
            "297/297 - 85s - loss: 0.1907 - val_loss: 0.4686\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.47062 to 0.46860, saving model to model.h5\n",
            "Epoch 59/100\n",
            "297/297 - 85s - loss: 0.1866 - val_loss: 0.4672\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.46860 to 0.46716, saving model to model.h5\n",
            "Epoch 60/100\n",
            "297/297 - 84s - loss: 0.1837 - val_loss: 0.4663\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.46716 to 0.46626, saving model to model.h5\n",
            "Epoch 61/100\n",
            "297/297 - 84s - loss: 0.1818 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.46626 to 0.46354, saving model to model.h5\n",
            "Epoch 62/100\n",
            "297/297 - 85s - loss: 0.1798 - val_loss: 0.4643\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.46354\n",
            "Epoch 63/100\n",
            "297/297 - 84s - loss: 0.1783 - val_loss: 0.4625\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.46354 to 0.46250, saving model to model.h5\n",
            "Epoch 64/100\n",
            "297/297 - 84s - loss: 0.1771 - val_loss: 0.4608\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.46250 to 0.46079, saving model to model.h5\n",
            "Epoch 65/100\n",
            "297/297 - 86s - loss: 0.1748 - val_loss: 0.4582\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.46079 to 0.45823, saving model to model.h5\n",
            "Epoch 66/100\n",
            "297/297 - 84s - loss: 0.1735 - val_loss: 0.4611\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.45823\n",
            "Epoch 67/100\n",
            "297/297 - 83s - loss: 0.1721 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.45823\n",
            "Epoch 68/100\n",
            "297/297 - 83s - loss: 0.1704 - val_loss: 0.4585\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.45823\n",
            "Epoch 69/100\n",
            "297/297 - 84s - loss: 0.1703 - val_loss: 0.4597\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.45823\n",
            "Epoch 70/100\n",
            "297/297 - 84s - loss: 0.1694 - val_loss: 0.4615\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.45823\n",
            "Epoch 71/100\n",
            "297/297 - 84s - loss: 0.1688 - val_loss: 0.4560\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.45823 to 0.45599, saving model to model.h5\n",
            "Epoch 72/100\n",
            "297/297 - 84s - loss: 0.1667 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.45599\n",
            "Epoch 73/100\n",
            "297/297 - 84s - loss: 0.1645 - val_loss: 0.4533\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.45599 to 0.45334, saving model to model.h5\n",
            "Epoch 74/100\n",
            "297/297 - 84s - loss: 0.1639 - val_loss: 0.4521\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.45334 to 0.45208, saving model to model.h5\n",
            "Epoch 75/100\n",
            "297/297 - 84s - loss: 0.1627 - val_loss: 0.4529\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.45208\n",
            "Epoch 76/100\n",
            "297/297 - 84s - loss: 0.1629 - val_loss: 0.4532\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.45208\n",
            "Epoch 77/100\n",
            "297/297 - 84s - loss: 0.1619 - val_loss: 0.4540\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.45208\n",
            "Epoch 78/100\n",
            "297/297 - 85s - loss: 0.1611 - val_loss: 0.4539\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.45208\n",
            "Epoch 79/100\n",
            "297/297 - 84s - loss: 0.1599 - val_loss: 0.4548\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.45208\n",
            "Epoch 80/100\n",
            "297/297 - 84s - loss: 0.1599 - val_loss: 0.4518\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.45208 to 0.45176, saving model to model.h5\n",
            "Epoch 81/100\n",
            "297/297 - 84s - loss: 0.1581 - val_loss: 0.4512\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.45176 to 0.45117, saving model to model.h5\n",
            "Epoch 82/100\n",
            "297/297 - 84s - loss: 0.1584 - val_loss: 0.4509\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.45117 to 0.45092, saving model to model.h5\n",
            "Epoch 83/100\n",
            "297/297 - 84s - loss: 0.1579 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.45092\n",
            "Epoch 84/100\n",
            "297/297 - 84s - loss: 0.1575 - val_loss: 0.4531\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.45092\n",
            "Epoch 85/100\n",
            "297/297 - 84s - loss: 0.1569 - val_loss: 0.4561\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.45092\n",
            "Epoch 86/100\n",
            "297/297 - 84s - loss: 0.1565 - val_loss: 0.4502\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.45092 to 0.45016, saving model to model.h5\n",
            "Epoch 87/100\n",
            "297/297 - 84s - loss: 0.1562 - val_loss: 0.4507\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.45016\n",
            "Epoch 88/100\n",
            "297/297 - 84s - loss: 0.1559 - val_loss: 0.4510\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.45016\n",
            "Epoch 89/100\n",
            "297/297 - 84s - loss: 0.1541 - val_loss: 0.4461\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.45016 to 0.44615, saving model to model.h5\n",
            "Epoch 90/100\n",
            "297/297 - 84s - loss: 0.1525 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.44615\n",
            "Epoch 91/100\n",
            "297/297 - 84s - loss: 0.1516 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.44615\n",
            "Epoch 92/100\n",
            "297/297 - 84s - loss: 0.1522 - val_loss: 0.4540\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.44615\n",
            "Epoch 93/100\n",
            "297/297 - 84s - loss: 0.1518 - val_loss: 0.4493\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.44615\n",
            "Epoch 94/100\n",
            "297/297 - 84s - loss: 0.1526 - val_loss: 0.4494\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.44615\n",
            "Epoch 95/100\n",
            "297/297 - 84s - loss: 0.1524 - val_loss: 0.4486\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.44615\n",
            "Epoch 96/100\n",
            "297/297 - 84s - loss: 0.1512 - val_loss: 0.4503\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.44615\n",
            "Epoch 97/100\n",
            "297/297 - 84s - loss: 0.1506 - val_loss: 0.4464\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.44615\n",
            "Epoch 98/100\n",
            "297/297 - 84s - loss: 0.1498 - val_loss: 0.4464\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.44615\n",
            "Epoch 99/100\n",
            "297/297 - 84s - loss: 0.1499 - val_loss: 0.4482\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.44615\n",
            "Epoch 100/100\n",
            "297/297 - 84s - loss: 0.1487 - val_loss: 0.4499\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.44615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc302471d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rtaa41OMCry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c542283f-ea1d-48ff-d440-dfde9e83ead3"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-japan-both.pkl')\n",
        "train = load_clean_sentences('english-japan-train.pkl')\n",
        "test = load_clean_sentences('english-japan-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[何が心配なの？], target=[Whats worrying you], predicted=[whats worrying you]\n",
            "src=[トムってまだ仕事してる？], target=[Is Tom still working], predicted=[is tom still working]\n",
            "src=[トムは実験が好きだ。], target=[Tom likes experimenting], predicted=[tom likes experimenting]\n",
            "src=[さあ仕事に取り掛かろう。], target=[Lets get down to work], predicted=[lets get down to work]\n",
            "src=[トムは泣いていた。], target=[Tom was crying], predicted=[tom cried]\n",
            "src=[あなたのことが大好きです。], target=[I like you a lot], predicted=[i like you a whole]\n",
            "src=[けが人はでなかった。], target=[Nobody was injured], predicted=[nobody was injured]\n",
            "src=[今日の新聞を見せて下さい。], target=[Show me todays paper], predicted=[show me todays paper]\n",
            "src=[どうぞ、お先に！], target=[Go ahead], predicted=[go ahead]\n",
            "src=[後ろにいてね。], target=[Stay behind me], predicted=[stay behind me]\n",
            "BLEU-1: 0.654450\n",
            "BLEU-2: 0.588466\n",
            "BLEU-3: 0.547806\n",
            "BLEU-4: 0.404921\n",
            "test\n",
            "src=[お薬ができましたよ。], target=[Your medicine is ready], predicted=[your medicine is ready]\n",
            "src=[ナイフが必要だ。], target=[I need a knife], predicted=[i need a knife]\n",
            "src=[それって、ここの近く？], target=[Is it near here], predicted=[is it near here]\n",
            "src=[６時まで待ちなさい。], target=[Wait till six], predicted=[wait till six]\n",
            "src=[彼は憎めない人だ。], target=[He is a lovable person], predicted=[he is a lovable person]\n",
            "src=[安全運転しなさい。], target=[Drive safely], predicted=[drive safely]\n",
            "src=[私、しゃべりすぎてる？], target=[Am I talking too much], predicted=[am i talking too much]\n",
            "src=[彼は私の級友です。], target=[He is my classmate], predicted=[he is my classmate]\n",
            "src=[ゆりは甘い香りがする。], target=[Lilies smell sweet], predicted=[lilies smell sweet]\n",
            "src=[あの音は何だったんだろう？], target=[What was that sound], predicted=[what was that sound]\n",
            "BLEU-1: 0.627074\n",
            "BLEU-2: 0.562965\n",
            "BLEU-3: 0.526183\n",
            "BLEU-4: 0.387811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZjdAw9Tgo_p",
        "outputId": "0c6acacc-3d09-409b-a163-8af4b5e0f8d3"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-japan-both.pkl')\n",
        "train = load_clean_sentences('english-japan-train.pkl')\n",
        "test = load_clean_sentences('english-japan-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[トムはメアリーに本を読んであげた。], target=[Tom read Mary a story], predicted=[tom read mary a story]\n",
            "src=[始めよければ半ば成功。], target=[Well begun is half done], predicted=[well begun is half done]\n",
            "src=[俺たち、夜通し飲んだよ。], target=[We drank all night], predicted=[we drank all night]\n",
            "src=[あの女の人は誰ですか。], target=[Whos that woman], predicted=[who that that]\n",
            "src=[トムは殺されたの？], target=[Was Tom killed], predicted=[was tom murdered]\n",
            "src=[彼女は彼を脅した。], target=[She threatened him], predicted=[she threatened him]\n",
            "src=[彼は来ないと思う。], target=[I think he wont come], predicted=[i think think hell come]\n",
            "src=[パーティーは終わった。], target=[The party is over], predicted=[the partys over]\n",
            "src=[この教科書はよい。], target=[This textbook is good], predicted=[this textbook is good]\n",
            "src=[誰がやったの？], target=[Who did it], predicted=[who did it]\n",
            "BLEU-1: 0.655966\n",
            "BLEU-2: 0.589633\n",
            "BLEU-3: 0.548459\n",
            "BLEU-4: 0.405037\n",
            "test\n",
            "src=[頼んだぞ。], target=[Im depending on you], predicted=[im depending on you]\n",
            "src=[私は勉強するつもりです。], target=[I am going to study], predicted=[i am going to study]\n",
            "src=[彼らは招待されなかった。], target=[They werent invited], predicted=[they werent invited]\n",
            "src=[あの女性たちは、気が強い。], target=[Those women are strong], predicted=[those women are strong]\n",
            "src=[すごく腹がへっている。], target=[I am terribly hungry], predicted=[i am terribly hungry]\n",
            "src=[トムは口が堅い。], target=[Tom can keep a secret], predicted=[tom can keep a secret]\n",
            "src=[彼女は何を持っていますか。], target=[What does she have], predicted=[what does she have]\n",
            "src=[何か見える？], target=[Can you see anything], predicted=[can you see something]\n",
            "src=[私達はどこで会いましょうか。], target=[Where shall we meet], predicted=[where will we meet]\n",
            "src=[ちょっとお待ち下さい。], target=[Please wait a minute], predicted=[hold wait a minute]\n",
            "BLEU-1: 0.627907\n",
            "BLEU-2: 0.563091\n",
            "BLEU-3: 0.525492\n",
            "BLEU-4: 0.386044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym9iirHW8a0L",
        "outputId": "6ba3bd17-8270-4c9b-9ba2-f8d1808e98dd"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-japan-both.pkl')\n",
        "train = load_clean_sentences('english-japan-train.pkl')\n",
        "test = load_clean_sentences('english-japan-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Japan Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Japan Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4207\n",
            "English Max Length: 7\n",
            "Japan Vocabulary Size: 16407\n",
            "Japan Max Length: 5\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 5, 256)            4200192   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 7, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 7, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 7, 4207)           1081199   \n",
            "=================================================================\n",
            "Total params: 6,332,015\n",
            "Trainable params: 6,332,015\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "297/297 - 70s - loss: 4.4105 - val_loss: 3.5876\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.58760, saving model to model.h5\n",
            "Epoch 2/200\n",
            "297/297 - 63s - loss: 3.5215 - val_loss: 3.3624\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.58760 to 3.36240, saving model to model.h5\n",
            "Epoch 3/200\n",
            "297/297 - 63s - loss: 3.3232 - val_loss: 3.2436\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.36240 to 3.24361, saving model to model.h5\n",
            "Epoch 4/200\n",
            "297/297 - 63s - loss: 3.2075 - val_loss: 3.1269\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.24361 to 3.12688, saving model to model.h5\n",
            "Epoch 5/200\n",
            "297/297 - 64s - loss: 3.0770 - val_loss: 3.0243\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.12688 to 3.02431, saving model to model.h5\n",
            "Epoch 6/200\n",
            "297/297 - 64s - loss: 2.9588 - val_loss: 2.9100\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02431 to 2.91003, saving model to model.h5\n",
            "Epoch 7/200\n",
            "297/297 - 64s - loss: 2.8458 - val_loss: 2.7976\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.91003 to 2.79755, saving model to model.h5\n",
            "Epoch 8/200\n",
            "297/297 - 65s - loss: 2.7046 - val_loss: 2.6517\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.79755 to 2.65174, saving model to model.h5\n",
            "Epoch 9/200\n",
            "297/297 - 64s - loss: 2.5580 - val_loss: 2.5180\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.65174 to 2.51798, saving model to model.h5\n",
            "Epoch 10/200\n",
            "297/297 - 63s - loss: 2.4207 - val_loss: 2.3797\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.51798 to 2.37967, saving model to model.h5\n",
            "Epoch 11/200\n",
            "297/297 - 64s - loss: 2.2800 - val_loss: 2.2399\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.37967 to 2.23988, saving model to model.h5\n",
            "Epoch 12/200\n",
            "297/297 - 63s - loss: 2.1411 - val_loss: 2.1151\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.23988 to 2.11506, saving model to model.h5\n",
            "Epoch 13/200\n",
            "297/297 - 64s - loss: 2.0141 - val_loss: 2.0041\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.11506 to 2.00413, saving model to model.h5\n",
            "Epoch 14/200\n",
            "297/297 - 64s - loss: 1.9003 - val_loss: 1.9009\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.00413 to 1.90094, saving model to model.h5\n",
            "Epoch 15/200\n",
            "297/297 - 64s - loss: 1.7945 - val_loss: 1.8024\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.90094 to 1.80242, saving model to model.h5\n",
            "Epoch 16/200\n",
            "297/297 - 63s - loss: 1.6925 - val_loss: 1.7102\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.80242 to 1.71024, saving model to model.h5\n",
            "Epoch 17/200\n",
            "297/297 - 64s - loss: 1.5953 - val_loss: 1.6252\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.71024 to 1.62519, saving model to model.h5\n",
            "Epoch 18/200\n",
            "297/297 - 65s - loss: 1.5009 - val_loss: 1.5443\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.62519 to 1.54429, saving model to model.h5\n",
            "Epoch 19/200\n",
            "297/297 - 64s - loss: 1.4131 - val_loss: 1.4599\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.54429 to 1.45985, saving model to model.h5\n",
            "Epoch 20/200\n",
            "297/297 - 64s - loss: 1.3248 - val_loss: 1.3814\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.45985 to 1.38136, saving model to model.h5\n",
            "Epoch 21/200\n",
            "297/297 - 64s - loss: 1.2411 - val_loss: 1.3062\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.38136 to 1.30619, saving model to model.h5\n",
            "Epoch 22/200\n",
            "297/297 - 63s - loss: 1.1600 - val_loss: 1.2390\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.30619 to 1.23897, saving model to model.h5\n",
            "Epoch 23/200\n",
            "297/297 - 64s - loss: 1.0834 - val_loss: 1.1724\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.23897 to 1.17244, saving model to model.h5\n",
            "Epoch 24/200\n",
            "297/297 - 64s - loss: 1.0108 - val_loss: 1.1118\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.17244 to 1.11180, saving model to model.h5\n",
            "Epoch 25/200\n",
            "297/297 - 64s - loss: 0.9411 - val_loss: 1.0500\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.11180 to 1.04997, saving model to model.h5\n",
            "Epoch 26/200\n",
            "297/297 - 63s - loss: 0.8748 - val_loss: 0.9952\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.04997 to 0.99516, saving model to model.h5\n",
            "Epoch 27/200\n",
            "297/297 - 64s - loss: 0.8130 - val_loss: 0.9430\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.99516 to 0.94304, saving model to model.h5\n",
            "Epoch 28/200\n",
            "297/297 - 64s - loss: 0.7524 - val_loss: 0.8917\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.94304 to 0.89170, saving model to model.h5\n",
            "Epoch 29/200\n",
            "297/297 - 64s - loss: 0.6983 - val_loss: 0.8473\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.89170 to 0.84727, saving model to model.h5\n",
            "Epoch 30/200\n",
            "297/297 - 64s - loss: 0.6462 - val_loss: 0.8068\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.84727 to 0.80683, saving model to model.h5\n",
            "Epoch 31/200\n",
            "297/297 - 64s - loss: 0.6006 - val_loss: 0.7690\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.80683 to 0.76903, saving model to model.h5\n",
            "Epoch 32/200\n",
            "297/297 - 64s - loss: 0.5567 - val_loss: 0.7354\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.76903 to 0.73544, saving model to model.h5\n",
            "Epoch 33/200\n",
            "297/297 - 64s - loss: 0.5163 - val_loss: 0.7058\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.73544 to 0.70578, saving model to model.h5\n",
            "Epoch 34/200\n",
            "297/297 - 64s - loss: 0.4807 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.70578 to 0.67774, saving model to model.h5\n",
            "Epoch 35/200\n",
            "297/297 - 64s - loss: 0.4464 - val_loss: 0.6497\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.67774 to 0.64973, saving model to model.h5\n",
            "Epoch 36/200\n",
            "297/297 - 64s - loss: 0.4157 - val_loss: 0.6298\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.64973 to 0.62977, saving model to model.h5\n",
            "Epoch 37/200\n",
            "297/297 - 64s - loss: 0.3891 - val_loss: 0.6075\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.62977 to 0.60750, saving model to model.h5\n",
            "Epoch 38/200\n",
            "297/297 - 64s - loss: 0.3642 - val_loss: 0.5887\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.60750 to 0.58874, saving model to model.h5\n",
            "Epoch 39/200\n",
            "297/297 - 64s - loss: 0.3422 - val_loss: 0.5721\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.58874 to 0.57209, saving model to model.h5\n",
            "Epoch 40/200\n",
            "297/297 - 63s - loss: 0.3228 - val_loss: 0.5586\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.57209 to 0.55861, saving model to model.h5\n",
            "Epoch 41/200\n",
            "297/297 - 64s - loss: 0.3051 - val_loss: 0.5475\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.55861 to 0.54745, saving model to model.h5\n",
            "Epoch 42/200\n",
            "297/297 - 64s - loss: 0.2897 - val_loss: 0.5352\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.54745 to 0.53516, saving model to model.h5\n",
            "Epoch 43/200\n",
            "297/297 - 64s - loss: 0.2758 - val_loss: 0.5242\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.53516 to 0.52422, saving model to model.h5\n",
            "Epoch 44/200\n",
            "297/297 - 63s - loss: 0.2633 - val_loss: 0.5176\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.52422 to 0.51763, saving model to model.h5\n",
            "Epoch 45/200\n",
            "297/297 - 63s - loss: 0.2521 - val_loss: 0.5113\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.51763 to 0.51129, saving model to model.h5\n",
            "Epoch 46/200\n",
            "297/297 - 63s - loss: 0.2432 - val_loss: 0.5055\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.51129 to 0.50550, saving model to model.h5\n",
            "Epoch 47/200\n",
            "297/297 - 63s - loss: 0.2359 - val_loss: 0.5005\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.50550 to 0.50048, saving model to model.h5\n",
            "Epoch 48/200\n",
            "297/297 - 63s - loss: 0.2282 - val_loss: 0.4935\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.50048 to 0.49349, saving model to model.h5\n",
            "Epoch 49/200\n",
            "297/297 - 63s - loss: 0.2220 - val_loss: 0.4888\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.49349 to 0.48881, saving model to model.h5\n",
            "Epoch 50/200\n",
            "297/297 - 63s - loss: 0.2157 - val_loss: 0.4859\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.48881 to 0.48587, saving model to model.h5\n",
            "Epoch 51/200\n",
            "297/297 - 63s - loss: 0.2101 - val_loss: 0.4873\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.48587\n",
            "Epoch 52/200\n",
            "297/297 - 63s - loss: 0.2055 - val_loss: 0.4815\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.48587 to 0.48146, saving model to model.h5\n",
            "Epoch 53/200\n",
            "297/297 - 63s - loss: 0.2021 - val_loss: 0.4784\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.48146 to 0.47836, saving model to model.h5\n",
            "Epoch 54/200\n",
            "297/297 - 63s - loss: 0.1995 - val_loss: 0.4776\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.47836 to 0.47763, saving model to model.h5\n",
            "Epoch 55/200\n",
            "297/297 - 63s - loss: 0.1945 - val_loss: 0.4770\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.47763 to 0.47701, saving model to model.h5\n",
            "Epoch 56/200\n",
            "297/297 - 63s - loss: 0.1916 - val_loss: 0.4748\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.47701 to 0.47479, saving model to model.h5\n",
            "Epoch 57/200\n",
            "297/297 - 63s - loss: 0.1879 - val_loss: 0.4732\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.47479 to 0.47323, saving model to model.h5\n",
            "Epoch 58/200\n",
            "297/297 - 63s - loss: 0.1864 - val_loss: 0.4711\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.47323 to 0.47107, saving model to model.h5\n",
            "Epoch 59/200\n",
            "297/297 - 63s - loss: 0.1842 - val_loss: 0.4706\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.47107 to 0.47056, saving model to model.h5\n",
            "Epoch 60/200\n",
            "297/297 - 63s - loss: 0.1828 - val_loss: 0.4727\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.47056\n",
            "Epoch 61/200\n",
            "297/297 - 63s - loss: 0.1797 - val_loss: 0.4705\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.47056 to 0.47050, saving model to model.h5\n",
            "Epoch 62/200\n",
            "297/297 - 63s - loss: 0.1783 - val_loss: 0.4722\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.47050\n",
            "Epoch 63/200\n",
            "297/297 - 63s - loss: 0.1772 - val_loss: 0.4663\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.47050 to 0.46625, saving model to model.h5\n",
            "Epoch 64/200\n",
            "297/297 - 63s - loss: 0.1753 - val_loss: 0.4676\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.46625\n",
            "Epoch 65/200\n",
            "297/297 - 63s - loss: 0.1740 - val_loss: 0.4664\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.46625\n",
            "Epoch 66/200\n",
            "297/297 - 63s - loss: 0.1733 - val_loss: 0.4650\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.46625 to 0.46500, saving model to model.h5\n",
            "Epoch 67/200\n",
            "297/297 - 63s - loss: 0.1711 - val_loss: 0.4650\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.46500 to 0.46496, saving model to model.h5\n",
            "Epoch 68/200\n",
            "297/297 - 63s - loss: 0.1697 - val_loss: 0.4613\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.46496 to 0.46126, saving model to model.h5\n",
            "Epoch 69/200\n",
            "297/297 - 63s - loss: 0.1682 - val_loss: 0.4629\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.46126\n",
            "Epoch 70/200\n",
            "297/297 - 63s - loss: 0.1671 - val_loss: 0.4623\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.46126\n",
            "Epoch 71/200\n",
            "297/297 - 63s - loss: 0.1656 - val_loss: 0.4618\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.46126\n",
            "Epoch 72/200\n",
            "297/297 - 63s - loss: 0.1653 - val_loss: 0.4607\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.46126 to 0.46068, saving model to model.h5\n",
            "Epoch 73/200\n",
            "297/297 - 63s - loss: 0.1646 - val_loss: 0.4638\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.46068\n",
            "Epoch 74/200\n",
            "297/297 - 63s - loss: 0.1634 - val_loss: 0.4628\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.46068\n",
            "Epoch 75/200\n",
            "297/297 - 63s - loss: 0.1639 - val_loss: 0.4639\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.46068\n",
            "Epoch 76/200\n",
            "297/297 - 63s - loss: 0.1621 - val_loss: 0.4614\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.46068\n",
            "Epoch 77/200\n",
            "297/297 - 63s - loss: 0.1608 - val_loss: 0.4615\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.46068\n",
            "Epoch 78/200\n",
            "297/297 - 63s - loss: 0.1607 - val_loss: 0.4625\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.46068\n",
            "Epoch 79/200\n",
            "297/297 - 63s - loss: 0.1605 - val_loss: 0.4587\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.46068 to 0.45871, saving model to model.h5\n",
            "Epoch 80/200\n",
            "297/297 - 64s - loss: 0.1584 - val_loss: 0.4594\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.45871\n",
            "Epoch 81/200\n",
            "297/297 - 63s - loss: 0.1586 - val_loss: 0.4603\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.45871\n",
            "Epoch 82/200\n",
            "297/297 - 63s - loss: 0.1573 - val_loss: 0.4639\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.45871\n",
            "Epoch 83/200\n",
            "297/297 - 63s - loss: 0.1577 - val_loss: 0.4567\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.45871 to 0.45672, saving model to model.h5\n",
            "Epoch 84/200\n",
            "297/297 - 63s - loss: 0.1567 - val_loss: 0.4576\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.45672\n",
            "Epoch 85/200\n",
            "297/297 - 63s - loss: 0.1568 - val_loss: 0.4579\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.45672\n",
            "Epoch 86/200\n",
            "297/297 - 63s - loss: 0.1542 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.45672\n",
            "Epoch 87/200\n",
            "297/297 - 63s - loss: 0.1550 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.45672\n",
            "Epoch 88/200\n",
            "297/297 - 63s - loss: 0.1540 - val_loss: 0.4596\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.45672\n",
            "Epoch 89/200\n",
            "297/297 - 63s - loss: 0.1543 - val_loss: 0.4626\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.45672\n",
            "Epoch 90/200\n",
            "297/297 - 63s - loss: 0.1541 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45672\n",
            "Epoch 91/200\n",
            "297/297 - 63s - loss: 0.1532 - val_loss: 0.4573\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.45672\n",
            "Epoch 92/200\n",
            "297/297 - 63s - loss: 0.1535 - val_loss: 0.4594\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.45672\n",
            "Epoch 93/200\n",
            "297/297 - 63s - loss: 0.1530 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.45672\n",
            "Epoch 94/200\n",
            "297/297 - 63s - loss: 0.1519 - val_loss: 0.4576\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45672\n",
            "Epoch 95/200\n",
            "297/297 - 62s - loss: 0.1517 - val_loss: 0.4597\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45672\n",
            "Epoch 96/200\n",
            "297/297 - 63s - loss: 0.1504 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45672\n",
            "Epoch 97/200\n",
            "297/297 - 63s - loss: 0.1507 - val_loss: 0.4603\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45672\n",
            "Epoch 98/200\n",
            "297/297 - 63s - loss: 0.1496 - val_loss: 0.4590\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45672\n",
            "Epoch 99/200\n",
            "297/297 - 63s - loss: 0.1494 - val_loss: 0.4562\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.45672 to 0.45624, saving model to model.h5\n",
            "Epoch 100/200\n",
            "297/297 - 63s - loss: 0.1479 - val_loss: 0.4575\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45624\n",
            "Epoch 101/200\n",
            "297/297 - 63s - loss: 0.1486 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.45624\n",
            "Epoch 102/200\n",
            "297/297 - 63s - loss: 0.1483 - val_loss: 0.4612\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.45624\n",
            "Epoch 103/200\n",
            "297/297 - 63s - loss: 0.1481 - val_loss: 0.4561\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.45624 to 0.45612, saving model to model.h5\n",
            "Epoch 104/200\n",
            "297/297 - 63s - loss: 0.1484 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.45612\n",
            "Epoch 105/200\n",
            "297/297 - 63s - loss: 0.1476 - val_loss: 0.4615\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.45612\n",
            "Epoch 106/200\n",
            "297/297 - 63s - loss: 0.1474 - val_loss: 0.4581\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.45612\n",
            "Epoch 107/200\n",
            "297/297 - 63s - loss: 0.1478 - val_loss: 0.4626\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.45612\n",
            "Epoch 108/200\n",
            "297/297 - 63s - loss: 0.1470 - val_loss: 0.4613\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.45612\n",
            "Epoch 109/200\n",
            "297/297 - 63s - loss: 0.1458 - val_loss: 0.4597\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.45612\n",
            "Epoch 110/200\n",
            "297/297 - 63s - loss: 0.1456 - val_loss: 0.4602\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.45612\n",
            "Epoch 111/200\n",
            "297/297 - 63s - loss: 0.1461 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.45612\n",
            "Epoch 112/200\n",
            "297/297 - 63s - loss: 0.1454 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.45612\n",
            "Epoch 113/200\n",
            "297/297 - 63s - loss: 0.1463 - val_loss: 0.4596\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.45612\n",
            "Epoch 114/200\n",
            "297/297 - 63s - loss: 0.1451 - val_loss: 0.4615\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.45612\n",
            "Epoch 115/200\n",
            "297/297 - 63s - loss: 0.1457 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.45612\n",
            "Epoch 116/200\n",
            "297/297 - 63s - loss: 0.1451 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.45612\n",
            "Epoch 117/200\n",
            "297/297 - 63s - loss: 0.1441 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.45612\n",
            "Epoch 118/200\n",
            "297/297 - 63s - loss: 0.1434 - val_loss: 0.4559\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.45612 to 0.45589, saving model to model.h5\n",
            "Epoch 119/200\n",
            "297/297 - 63s - loss: 0.1427 - val_loss: 0.4607\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.45589\n",
            "Epoch 120/200\n",
            "297/297 - 63s - loss: 0.1421 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.45589\n",
            "Epoch 121/200\n",
            "297/297 - 63s - loss: 0.1429 - val_loss: 0.4569\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.45589\n",
            "Epoch 122/200\n",
            "297/297 - 63s - loss: 0.1430 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.45589\n",
            "Epoch 123/200\n",
            "297/297 - 63s - loss: 0.1428 - val_loss: 0.4611\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.45589\n",
            "Epoch 124/200\n",
            "297/297 - 63s - loss: 0.1428 - val_loss: 0.4566\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.45589\n",
            "Epoch 125/200\n",
            "297/297 - 63s - loss: 0.1419 - val_loss: 0.4609\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.45589\n",
            "Epoch 126/200\n",
            "297/297 - 62s - loss: 0.1417 - val_loss: 0.4590\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.45589\n",
            "Epoch 127/200\n",
            "297/297 - 62s - loss: 0.1415 - val_loss: 0.4574\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.45589\n",
            "Epoch 128/200\n",
            "297/297 - 63s - loss: 0.1412 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.45589\n",
            "Epoch 129/200\n",
            "297/297 - 63s - loss: 0.1413 - val_loss: 0.4618\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45589\n",
            "Epoch 130/200\n",
            "297/297 - 63s - loss: 0.1415 - val_loss: 0.4587\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.45589\n",
            "Epoch 131/200\n",
            "297/297 - 63s - loss: 0.1409 - val_loss: 0.4612\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.45589\n",
            "Epoch 132/200\n",
            "297/297 - 63s - loss: 0.1418 - val_loss: 0.4607\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.45589\n",
            "Epoch 133/200\n",
            "297/297 - 63s - loss: 0.1419 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.45589\n",
            "Epoch 134/200\n",
            "297/297 - 63s - loss: 0.1409 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.45589\n",
            "Epoch 135/200\n",
            "297/297 - 62s - loss: 0.1399 - val_loss: 0.4582\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.45589\n",
            "Epoch 136/200\n",
            "297/297 - 63s - loss: 0.1399 - val_loss: 0.4575\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45589\n",
            "Epoch 137/200\n",
            "297/297 - 63s - loss: 0.1403 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45589\n",
            "Epoch 138/200\n",
            "297/297 - 62s - loss: 0.1398 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45589\n",
            "Epoch 139/200\n",
            "297/297 - 63s - loss: 0.1391 - val_loss: 0.4579\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45589\n",
            "Epoch 140/200\n",
            "297/297 - 63s - loss: 0.1387 - val_loss: 0.4583\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.45589\n",
            "Epoch 141/200\n",
            "297/297 - 63s - loss: 0.1392 - val_loss: 0.4586\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45589\n",
            "Epoch 142/200\n",
            "297/297 - 62s - loss: 0.1390 - val_loss: 0.4591\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.45589\n",
            "Epoch 143/200\n",
            "297/297 - 63s - loss: 0.1389 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.45589\n",
            "Epoch 144/200\n",
            "297/297 - 63s - loss: 0.1392 - val_loss: 0.4575\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.45589\n",
            "Epoch 145/200\n",
            "297/297 - 62s - loss: 0.1387 - val_loss: 0.4611\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.45589\n",
            "Epoch 146/200\n",
            "297/297 - 63s - loss: 0.1383 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.45589\n",
            "Epoch 147/200\n",
            "297/297 - 62s - loss: 0.1373 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.45589\n",
            "Epoch 148/200\n",
            "297/297 - 62s - loss: 0.1382 - val_loss: 0.4586\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.45589\n",
            "Epoch 149/200\n",
            "297/297 - 63s - loss: 0.1377 - val_loss: 0.4600\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.45589\n",
            "Epoch 150/200\n",
            "297/297 - 63s - loss: 0.1380 - val_loss: 0.4624\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.45589\n",
            "Epoch 151/200\n",
            "297/297 - 63s - loss: 0.1380 - val_loss: 0.4623\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.45589\n",
            "Epoch 152/200\n",
            "297/297 - 62s - loss: 0.1375 - val_loss: 0.4602\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.45589\n",
            "Epoch 153/200\n",
            "297/297 - 62s - loss: 0.1372 - val_loss: 0.4598\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.45589\n",
            "Epoch 154/200\n",
            "297/297 - 63s - loss: 0.1367 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.45589\n",
            "Epoch 155/200\n",
            "297/297 - 63s - loss: 0.1365 - val_loss: 0.4600\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.45589\n",
            "Epoch 156/200\n",
            "297/297 - 63s - loss: 0.1367 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.45589\n",
            "Epoch 157/200\n",
            "297/297 - 63s - loss: 0.1377 - val_loss: 0.4569\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.45589\n",
            "Epoch 158/200\n",
            "297/297 - 63s - loss: 0.1375 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.45589\n",
            "Epoch 159/200\n",
            "297/297 - 63s - loss: 0.1369 - val_loss: 0.4610\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.45589\n",
            "Epoch 160/200\n",
            "297/297 - 63s - loss: 0.1368 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.45589\n",
            "Epoch 161/200\n",
            "297/297 - 62s - loss: 0.1371 - val_loss: 0.4595\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.45589\n",
            "Epoch 162/200\n",
            "297/297 - 63s - loss: 0.1363 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.45589\n",
            "Epoch 163/200\n",
            "297/297 - 63s - loss: 0.1355 - val_loss: 0.4629\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.45589\n",
            "Epoch 164/200\n",
            "297/297 - 63s - loss: 0.1353 - val_loss: 0.4604\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.45589\n",
            "Epoch 165/200\n",
            "297/297 - 62s - loss: 0.1351 - val_loss: 0.4604\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.45589\n",
            "Epoch 166/200\n",
            "297/297 - 63s - loss: 0.1346 - val_loss: 0.4593\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.45589\n",
            "Epoch 167/200\n",
            "297/297 - 63s - loss: 0.1349 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.45589\n",
            "Epoch 168/200\n",
            "297/297 - 63s - loss: 0.1351 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.45589\n",
            "Epoch 169/200\n",
            "297/297 - 63s - loss: 0.1348 - val_loss: 0.4586\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.45589\n",
            "Epoch 170/200\n",
            "297/297 - 63s - loss: 0.1347 - val_loss: 0.4601\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.45589\n",
            "Epoch 171/200\n",
            "297/297 - 63s - loss: 0.1353 - val_loss: 0.4603\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.45589\n",
            "Epoch 172/200\n",
            "297/297 - 63s - loss: 0.1355 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.45589\n",
            "Epoch 173/200\n",
            "297/297 - 63s - loss: 0.1360 - val_loss: 0.4598\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.45589\n",
            "Epoch 174/200\n",
            "297/297 - 63s - loss: 0.1353 - val_loss: 0.4617\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.45589\n",
            "Epoch 175/200\n",
            "297/297 - 63s - loss: 0.1350 - val_loss: 0.4574\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.45589\n",
            "Epoch 176/200\n",
            "297/297 - 63s - loss: 0.1346 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.45589\n",
            "Epoch 177/200\n",
            "297/297 - 63s - loss: 0.1364 - val_loss: 0.4585\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.45589\n",
            "Epoch 178/200\n",
            "297/297 - 62s - loss: 0.1359 - val_loss: 0.4572\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45589\n",
            "Epoch 179/200\n",
            "297/297 - 63s - loss: 0.1340 - val_loss: 0.4569\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.45589\n",
            "Epoch 180/200\n",
            "297/297 - 62s - loss: 0.1335 - val_loss: 0.4553\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.45589 to 0.45532, saving model to model.h5\n",
            "Epoch 181/200\n",
            "297/297 - 62s - loss: 0.1334 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.45532\n",
            "Epoch 182/200\n",
            "297/297 - 62s - loss: 0.1329 - val_loss: 0.4582\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.45532\n",
            "Epoch 183/200\n",
            "297/297 - 62s - loss: 0.1323 - val_loss: 0.4594\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.45532\n",
            "Epoch 184/200\n",
            "297/297 - 62s - loss: 0.1326 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.45532\n",
            "Epoch 185/200\n",
            "297/297 - 62s - loss: 0.1328 - val_loss: 0.4585\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.45532\n",
            "Epoch 186/200\n",
            "297/297 - 62s - loss: 0.1328 - val_loss: 0.4577\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.45532\n",
            "Epoch 187/200\n",
            "297/297 - 63s - loss: 0.1324 - val_loss: 0.4565\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.45532\n",
            "Epoch 188/200\n",
            "297/297 - 62s - loss: 0.1330 - val_loss: 0.4586\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.45532\n",
            "Epoch 189/200\n",
            "297/297 - 62s - loss: 0.1328 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.45532\n",
            "Epoch 190/200\n",
            "297/297 - 63s - loss: 0.1332 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.45532\n",
            "Epoch 191/200\n",
            "297/297 - 63s - loss: 0.1332 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.45532\n",
            "Epoch 192/200\n",
            "297/297 - 62s - loss: 0.1335 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.45532\n",
            "Epoch 193/200\n",
            "297/297 - 63s - loss: 0.1332 - val_loss: 0.4582\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.45532\n",
            "Epoch 194/200\n",
            "297/297 - 62s - loss: 0.1328 - val_loss: 0.4590\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.45532\n",
            "Epoch 195/200\n",
            "297/297 - 62s - loss: 0.1325 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.45532\n",
            "Epoch 196/200\n",
            "297/297 - 62s - loss: 0.1324 - val_loss: 0.4579\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.45532\n",
            "Epoch 197/200\n",
            "297/297 - 62s - loss: 0.1319 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.45532\n",
            "Epoch 198/200\n",
            "297/297 - 63s - loss: 0.1321 - val_loss: 0.4591\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.45532\n",
            "Epoch 199/200\n",
            "297/297 - 63s - loss: 0.1323 - val_loss: 0.4591\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.45532\n",
            "Epoch 200/200\n",
            "297/297 - 63s - loss: 0.1322 - val_loss: 0.4573\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.45532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f61861f5d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsMHJ-fw9rY7",
        "outputId": "a96f8f6e-1dbb-4c3a-f577-689547c834b2"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-japan-both.pkl')\n",
        "train = load_clean_sentences('english-japan-train.pkl')\n",
        "test = load_clean_sentences('english-japan-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[人の好みは異なる。], target=[Tastes differ], predicted=[tastes differ]\n",
            "src=[2時半頃出た。], target=[I left around 230], predicted=[i left around 230]\n",
            "src=[トムのお父ちゃん、リッチだよな。], target=[Toms dad is rich], predicted=[toms dad is rich]\n",
            "src=[お会いできて光栄です。], target=[Im charmed to meet you], predicted=[im glad to meet you]\n",
            "src=[テーブルの上には猫がいた。], target=[A cat was on the table], predicted=[a cat was on the table]\n",
            "src=[ひどく痛いの？], target=[Does it hurt a lot], predicted=[are you in a lot]\n",
            "src=[泳げますか。], target=[Can you swim], predicted=[can you swim]\n",
            "src=[彼女は彼より頭が切れる。], target=[Shes smarter than him], predicted=[shes smarter than him]\n",
            "src=[両親は離婚しました。], target=[My parents are divorced], predicted=[my parents are divorced]\n",
            "src=[声を出して読みなさい。], target=[Read it aloud], predicted=[read it aloud]\n",
            "BLEU-1: 0.656199\n",
            "BLEU-2: 0.590023\n",
            "BLEU-3: 0.549085\n",
            "BLEU-4: 0.406137\n",
            "test\n",
            "src=[覚えていない。], target=[I dont recall], predicted=[i dont recall]\n",
            "src=[トムを呼んで。], target=[Call Tom], predicted=[call tom]\n",
            "src=[これはドリルではない。], target=[This isnt a drill], predicted=[this isnt a a]\n",
            "src=[もうここらでいい加減勘弁して。], target=[Give me a break here], predicted=[give me a break here]\n",
            "src=[彼は学生らしい。], target=[He seems to be a student], predicted=[he seems to be a student]\n",
            "src=[時間に遅れないで。], target=[Be on time], predicted=[be on time]\n",
            "src=[私はもう彼に会わないだろう。], target=[I will never see him], predicted=[i will never see him]\n",
            "src=[お年寄りにはやさしくしなさい。], target=[Be kind to old people], predicted=[be kind to old people]\n",
            "src=[意気地がなかったんだ。], target=[I wimped out], predicted=[i wimped out]\n",
            "src=[わかりません。], target=[I dont know], predicted=[i dont know]\n",
            "BLEU-1: 0.628670\n",
            "BLEU-2: 0.564273\n",
            "BLEU-3: 0.527075\n",
            "BLEU-4: 0.388496\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}